---
title: "#3 XCiT: Cross-Covariance Image Transformers (Facebook AI)"
collection: talks
type: "Research Paper Explanation Video"
permalink: /talks/Jun 23, 2021-XCiT
venue: "YouTube"
date: "June 23, 2021"
location: "channel - Sahil Khose"
---

After dominating Natural Language Processing, Transformers have taken over Computer Vision recently with the advent of Vision Transformers. However, the attention mechanism's quadratic complexity in the number of tokens means that Transformers do not scale well to high-resolution images. XCiT is a new Transformer architecture, containing XCA, a transposed version of attention, reducing the complexity from quadratic to linear, and at least on image data, it appears to perform on par with other models. What does this mean for the field? Is this even a transformer? What really matters in deep learning? [Link for the video](https://www.youtube.com/watch?v=FVLf9agw8Ho)
