---
title: "#14 Self-Distillation: Towards Efficient and Compact Neural Networks"
collection: talks
type: "Research Paper Explanation Video"
permalink: /talks/Sep 05, 2021-Self_Distillation
venue: "YouTube"
date: "Sep 05, 2021"
location: "channel - Sahil Khose"
---

Instead of transferring knowledge from a teacher model to a different student model, self-distillation does it in the same model! This approach leads to faster inference and smaller models! [Link for the video](https://www.youtube.com/watch?v=ugvHJbzhod8)
